{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5y98twIUC-T"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP_2022/blob/main/03%20Machine%20Learning/notebooks/14-Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdQaBQt7UC-X"
      },
      "source": [
        "<h1>Clustering</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpuaRVvnUC-c"
      },
      "source": [
        "* El análisis de agrupamiento, o agrupamiento, es una tarea de aprendizaje automático no supervisada.\n",
        "\n",
        "* Implica descubrir automáticamente la agrupación natural de los datos. A diferencia del aprendizaje supervisado (como el modelado predictivo), los algoritmos de agrupación solo interpretan los datos de entrada y encuentran grupos o agrupaciones naturales en el espacio de características.\n",
        "\n",
        "* Un grupo, o cluster, es un en el espacio de características donde las instancias están más cerca del grupo que de otros grupos.\n",
        "\n",
        "* Es probable que estos grupos reflejen algún mecanismo en funcionamiento en el dominio del que se extraen las instancias, un mecanismo que hace que algunas instancias tengan un parecido más fuerte entre sí que con las instancias restantes.\n",
        "\n",
        "* La agrupación en clústers puede ser útil como actividad de análisis de datos para obtener más información sobre el dominio del problema, el llamado descubrimiento de patrones o descubrimiento de conocimiento.\n",
        "\n",
        "* El agrupamiento también puede ser útil como un tipo de ingeniería de características, donde los ejemplos existentes y nuevos se pueden mapear y etiquetar como pertenecientes a uno de los grupos identificados en los datos.\n",
        "\n",
        "* La evaluación de los grupos identificados es subjetiva y puede requerir un experto en el dominio, aunque existen muchas medidas cuantitativas específicas de los grupos.\n",
        "\n",
        "&#128214; <u>Referencias bibliográficas</u>:\n",
        "* Flach, Peter (2012). Machine Learning: The Art and Science of Algorithms that Make Sense of Data. Cambridge University Press.\n",
        "\n",
        "[Algoritmos de clustering en scikit-learn](https://scikit-learn.org/stable/modules/clustering.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy_dshVLUC-g"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrhsqwc9UC-j"
      },
      "source": [
        "# Demostración ilustrativa de algunos algoritmos de agrupamiento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN, OPTICS, AgglomerativeClustering"
      ],
      "metadata": {
        "id": "bMNIcQ3lhySs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5cjPkORUC-k"
      },
      "source": [
        "Generación de datos sintéticos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIrvqFYVUC-k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# X, y = make_blobs(n_samples=500,centers=3, random_state=24)\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:, 0], X[:, 1],c=y)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferencia del aprendizaje supervisado, en los métodos de clustering (implementados en scikit-learn) el proceso es de la siguiente manera:\n",
        "\n",
        "1. Inicializar el objeto, por ejemplo `modelo = KMeans(n_clusters=3)`.\n",
        "2. Hacer fit: `modelo.fit(X)`. Hay dos opciones:\n",
        "    * Obtener la lista de etiquetas de clusters como `y_clusters = modelo.predict(X)`.\n",
        "    * Obtener la lista de etiquetas de clusters usando el atributo `labels_` como `y_clusters = modelo.labels_`.\n",
        "\n",
        "También pueden obtenerse las etiquetas de los clusters directamente con el método `fit_transform()`.\n",
        "\n",
        "En esta notebook estaremos usando indistintamente los 3 métodos para ejemplificar su uso."
      ],
      "metadata": {
        "id": "-HKVaJOGLe8e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XewRUibwUC-u"
      },
      "source": [
        "## [K-MEANS](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probemos varios valores de `n_clusters`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s_I4uuysgJbU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diS9kzsmUC-u"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "modelo = KMeans(n_clusters=3)\n",
        "\n",
        "modelo.fit(X)\n",
        "y_clusters = modelo.predict(X)\n",
        "\n",
        "clusters = np.unique(y_clusters)\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(y_clusters == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos los centroides de cada cluster"
      ],
      "metadata": {
        "id": "PyyDVjX57DQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centers = modelo.cluster_centers_\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1],c=y_clusters)\n",
        "plt.scatter(centers[:,0],centers[:,1],color='black', marker='x',s=80)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2N1VQf9z6ZUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¿Cómo sabemos cuántos clusters buscar?\n",
        "\n",
        "Exploremos el **elbow value**.\n",
        "\n",
        "Para esto, hay que graficar los valores de inercia de cada módelo de K-means para varios valores de $k$. El valor *elbow* representa el número óptimo de clusters, queda determinado por el *codo* en la curva graficada."
      ],
      "metadata": {
        "id": "NMispnH_B-24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_num_clusters = 20\n",
        "\n",
        "inertias = []\n",
        "k_values = list(range(1,max_num_clusters))\n",
        "for k in k_values:\n",
        "    modelo = KMeans(n_clusters=k, n_init='auto')\n",
        "    modelo.fit(X)\n",
        "    inertias.append(modelo.inertia_)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,inertias,color='red')\n",
        "plt.axvline(x=3,linestyle='dashed',color='gray')\n",
        "plt.ylabel(\"Inertia\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.xticks(k_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ugk8MAJT7IuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Silhoutte Score**\n",
        "\n",
        "También podemos usar el **score de silueta**, el cual es un valor $-1\\leq s\\leq 1$ que mide que tan coherente son los puntos dentro de sus propios clusters, en términos de las distancias a los demás clusters. Entre más alto el valor, la configuración del clúster es apropiada. Este score es intrínseco del clustering.\n",
        "\n",
        "Usaremos la implementación de [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)."
      ],
      "metadata": {
        "id": "XIArcI2_ECzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "max_num_clusters = 20\n",
        "\n",
        "siluetas = []\n",
        "k_values = list(range(2,max_num_clusters))\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto').fit(X)\n",
        "    labels = kmeans.labels_\n",
        "    siluetas.append(silhouette_score(X, labels, metric='euclidean'))\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,siluetas,color='red')\n",
        "plt.axvline(x=3,linestyle='dashed',color='gray')\n",
        "plt.xticks(k_values)\n",
        "plt.ylabel(\"Silhoutte Scores\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A7mTsL6mDQXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adjusted Mutual Information (AMI)**\n",
        "\n",
        "Es un score que asigna una puntuación a la comparación entre dos clusterings. La Información Mutua Ajustada mide que tanta información comparten dos clusterings en términos de los elementos que comparten, es decir, del tamaño de la intersección.\n",
        "\n",
        "Suele usarse para comparar un clustering *ground truth* contra uno que hemos obtenido. **Es forzoso tener dos clusterings.**\n",
        "\n",
        "Usaremos la implementación de [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2A2Vmdrozb63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_mutual_info_score\n",
        "\n",
        "max_num_clusters = 20\n",
        "\n",
        "scores = []\n",
        "k_values = list(range(2,max_num_clusters))\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto')\n",
        "    kmeans.fit(X)\n",
        "    labels = kmeans.labels_\n",
        "    scores.append(adjusted_mutual_info_score(y, labels))\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,scores,color='red')\n",
        "plt.axvline(x=3,linestyle='dashed',color='gray')\n",
        "plt.xticks(k_values)\n",
        "plt.ylabel(\"Adjusted Mutual Information\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dfYSiusEy5_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, en los tres casos se valida la hipótesis de que el mejor valor para $K$ es $K=3$. Esta hipótesis tiene mayor peso por el conocimiento previo del problema, es decir, al generar los datos sabiamos que teniamos 3 grupos de puntos."
      ],
      "metadata": {
        "id": "AYVmcBtfEq1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Means es sensible a outliers"
      ],
      "metadata": {
        "id": "9RH5O2sb2awy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=500,centers=3, random_state=174)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DpExNQY52XnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xp = X.copy()\n",
        "\n",
        "Xp[2] = Xp[2] + [4,0]\n",
        "Xp[5] = Xp[5] + [4.25,-1]\n",
        "Xp[1] = Xp[1] + [-3,0]\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(Xp[:,0],Xp[:,1],c=y)\n",
        "plt.scatter(Xp[[1,2,5],0],Xp[[1,2,5],1],marker='x',color='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O9Tkyiyc27o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "kmeans_p = KMeans(n_clusters=3)\n",
        "kmeans_p.fit(Xp)\n",
        "labels_p = kmeans_p.labels_\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(9,5),sharey=True)\n",
        "axs[0].scatter(X[:,0],X[:,1],c=labels)\n",
        "axs[1].scatter(Xp[:,0],Xp[:,1],c=labels_p)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "FMz7ly403R7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "max_num_clusters = 20\n",
        "\n",
        "siluetas_1 = []\n",
        "siluetas_2 = []\n",
        "k_values = list(range(2,max_num_clusters))\n",
        "for k in k_values:\n",
        "    kmeans_1 = KMeans(n_clusters=k, n_init='auto').fit(X)\n",
        "    labels_1 = kmeans_1.labels_\n",
        "    siluetas_1.append(silhouette_score(X, labels_1, metric='euclidean'))\n",
        "    kmeans_2 = KMeans(n_clusters=k, n_init='auto').fit(X)\n",
        "    labels_2 = kmeans_2.labels_\n",
        "    siluetas_2.append(silhouette_score(Xp, labels_2, metric='euclidean'))\n",
        "\n",
        "fig, axs = plt.subplots(1,2)\n",
        "fig.suptitle(\"Silhoutte Scores\")\n",
        "axs[0].plot(k_values,siluetas_1,color='red')\n",
        "axs[0].set_title(\"Original dataset\")\n",
        "axs[1].plot(k_values,siluetas_2,color='red')\n",
        "axs[1].set_title(\"Con outliers\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uhgLD5P074NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT3pieSEUC-p"
      },
      "source": [
        "## [Clustering Jerárquico](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
        "\n",
        "* La agrupación jerárquica es una familia general de algoritmos de agrupación que crean agrupaciones anidadas fusionándolas o dividiéndolas sucesivamente. Esta jerarquía de grupos se representa como un árbol (o dendrograma). La raíz del árbol es el grupo único que reúne todas las muestras, siendo las hojas los grupos con una sola muestra.\n",
        "* La implementación [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) de scikit-learn realiza una agrupación jerárquica utilizando un enfoque ascendente (bottom-up): cada observación comienza en su propio grupo, y los grupos se fusionan sucesivamente. Los criterios de vinculación (_linkage_) determinan la métrica utilizada para la estrategia de fusión:\n",
        "  - _Ward_ minimiza la suma de las diferencias al cuadrado dentro de todos los grupos. Es un enfoque que minimiza la varianza y, en este sentido, es similar a la función objetivo de k-means pero se aborda con un enfoque jerárquico aglomerativo.\n",
        "  - _Maximum_ o _complete Linkage_ minimiza la distancia máxima entre observaciones de pares de grupos.\n",
        "  - _Average linkage_ minimiza el promedio de las distancias entre todas las observaciones de pares de grupos.\n",
        "  - _Single linkage_ minimiza la distancia entre las observaciones más cercanas de pares de grupos.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Se puede especificar, ya sea el número de clusters o el umbral de distancia máxima:\n",
        "\n",
        "* Si `n_clusters`$\\geq2$ entonces nos regresa ese número de clusters.\n",
        "* Si `n_clusters`=None, hay que especificar un `distance_threshold`.\n",
        "* Si `distance_threshold`$\\neq$None, `n_clusters` debe ser None y `compute_full_tree` debe ser True.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIZ_4p5HUC-q"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# modelo = AgglomerativeClustering(n_clusters=3)\n",
        "# no_dendo = True\n",
        "\n",
        "# descomenta la siguiente línea si quieres ver un dendograma\n",
        "modelo = AgglomerativeClustering(distance_threshold=5.0,\n",
        "                                 n_clusters=None,\n",
        "                                 compute_full_tree=True)\n",
        "no_dendo=False\n",
        "\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(yhat == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "plt.show()\n",
        "\n",
        "print(f\"{len(clusters)} clusters encontrados.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "También podemos usar criterios externos para escoger un número de clusters adecuado. Por ejemplo, podemos usar el score de silueta. La conclusión es usar 3 clusters."
      ],
      "metadata": {
        "id": "6SeUcJL4H6VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "max_num_clusters = 20\n",
        "\n",
        "siluetas = []\n",
        "k_values = list(range(2,max_num_clusters))\n",
        "for k in k_values:\n",
        "    ac = AgglomerativeClustering(n_clusters=k)\n",
        "    ac.fit(X)\n",
        "    labels = ac.labels_\n",
        "    siluetas.append(silhouette_score(X, labels, metric='euclidean'))\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(k_values,siluetas,color='red')\n",
        "plt.axvline(x=3,linestyle='dashed',color='gray')\n",
        "plt.xticks(k_values)\n",
        "plt.ylabel(\"Silhoutte Scores\", fontsize=15)\n",
        "plt.xlabel(\"Value of k\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "okjT7LBkHfle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMSw79gBUC-r"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
        "                                      counts]).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "if not no_dendo:\n",
        "    plt.figure(dpi=120)\n",
        "    plt.title('Hierarchical Clustering Dendrogram')\n",
        "    # plot the top three levels of the dendrogram\n",
        "    plot_dendrogram(modelo, truncate_mode='level', p=3)\n",
        "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JogoUSPjUC-t"
      },
      "source": [
        "## [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
        "* [Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu (1996). _A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise_. Proceedings of Knowledge Discovery and Databases - The International Conference on Knowledge Discovery & Data Mining.](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)\n",
        "* DBSCAN recurre a una noción de cúmulos basada en la densidad de los mismos, que está diseñada para descubrir grupos de formas arbitrarias. DBSCAN requiere solo un parámetro de entrada $\\varepsilon$, el cual determina la distancia máxima entre dos puntos para considerarse cercanos. El otro parámetro importante es el `min_samples` el cual representa el número mínimo de puntos que puede haber en un cluster.\n",
        "\n",
        "* DBSCAN reconoce ruido. Es decir, puede dejar puntos sin asignar a ningún cluster.\n",
        "\n",
        "* \"_La razón principal por la que reconocemos los grupos, es que dentro de cada grupo tenemos una densidad típica de puntos que es considerablemente más alta que fuera del grupo. Además, la densidad dentro de las áreas de ruido es menor que la densidad en cualquiera de los grupos._\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1LX9VnhUC-t"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "modelo = DBSCAN(eps=0.530, min_samples=9)\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = [j for j in np.unique(yhat) if j!=-1]\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[yhat==-1,0],X[yhat==-1,1],marker='x',color='gray',label='ruido')\n",
        "for cluster in clusters:\n",
        "    filas = np.where(yhat == cluster)\n",
        "    plt.scatter(X[filas, 0], X[filas, 1])\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una particularidad de DBSCAN es la susceptibilidad de los resultados en función del principal parámetro $\\varepsilon$."
      ],
      "metadata": {
        "id": "Ldbjw07gv4sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valores_eps = np.linspace(0.0001,10,100)\n",
        "num_clusters = []\n",
        "\n",
        "for eps in valores_eps:\n",
        "    modelo = DBSCAN(eps=eps, min_samples=5)\n",
        "    yhat = modelo.fit_predict(X)\n",
        "    clusters = np.unique(yhat)\n",
        "    num_clusters.append(len(clusters))\n",
        "\n",
        "plt.plot(valores_eps,num_clusters)\n",
        "plt.suptitle(\"Número de clusters en función del parámetro eps\")\n",
        "plt.xlabel(\"eps\",fontsize=15)\n",
        "plt.ylabel(\"Número de clusters\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "etCJvkj_Rcc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Otros métodos"
      ],
      "metadata": {
        "id": "68s4pOjOw8mF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PeZ1RewUC-m"
      },
      "source": [
        "### [Affinity Propagation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html)\n",
        "\n",
        "* [Brendan J. Frey, Delbert Dueck (2007). Clustering by passing messages between data points. Science 315 (5814); pp. 972-6.](https://pdfs.semanticscholar.org/ea78/2c8b0848987e9575ea648e0419054d3f5bbf.pdf?_ga=2.62870572.1030401696.1591021245-1055786045.1581021538)\n",
        "\n",
        "* AffinityPropagation crea clusters enviando mensajes entre pares de muestras hasta la convergencia. Los mensajes enviados entre pares representan la idoneidad de una muestra para ser el ejemplar de la otra, que se actualiza en respuesta a los valores de otros pares. Esta actualización se produce de forma iterativa hasta la convergencia, momento en el que se eligen los ejemplares definitivos y, por tanto, se obtiene la agrupación final.\n",
        "\n",
        "* AffinityPropagation puede ser interesante, ya que elige el número de clusters en función de los datos proporcionados. Para ello, los dos parámetros importantes son la preferencia, que controla cuántos ejemplares se utilizan, y el factor de amortiguación, que amortigua los mensajes de responsabilidad y disponibilidad para evitar oscilaciones numéricas al actualizar estos mensajes.\n",
        "\n",
        "* El principal inconveniente de la propagación por afinidad es su complejidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEEowNZbUC-o"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AffinityPropagation\n",
        "\n",
        "modelo = AffinityPropagation(damping=0.5,max_iter=500,random_state=None)\n",
        "\n",
        "modelo.fit(X)\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "print(f\"Se encontraron {clusters.shape[0]} clusters\")\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(yhat == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ywQ1YzUC-r"
      },
      "source": [
        "### [BIRCH](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html)\n",
        "* Tian Zhang, Raghu Ramakrishnan and Miron Livny (1996). _BIRCH: An Efficient Data Clustering Method for Very Large Databases_.  ACM SIGMOD Record. DOI:10.1145/235968.233324.\n",
        "* BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) es un método de clustering de tipo jerárquico bottom-up, especialmente diseñado para grandes bases de datos.\n",
        "* BIRCH agrupa de forma incremental y dinámica datos métricos (que satisfacen los requisitos de una métrica Euclidiana) multidimensionales de entrada para intentar producir la mejor calidad de agrupación con los recursos disponibles (es decir, memoria disponible y limitaciones de tiempo).\n",
        "* BIRCH es también el primer algoritmo de agrupamiento propuesto en el área de base de datos para manejar ruido (puntos que no son parte del patrón subyacente) de manera efectiva.\n",
        "* El algoritmo BIRCH se basa en la construcción iterativa de un árbol cuyos nodos son tripletas (llamadas Clustering Features -- $CF$) que resumen la información acerca de un cluster; $CF=(N,LS,SS)$, donde $N$ es el número de puntos en un cluster, LS es la suma lineal de los N puntos ($LS=\\sum_i X_i$) y $SS$ es la suma al cuadrado de los N puntos ($SS=\\sum_i X_i^2$). La idea de CF es mantener una representación compacta de los clusters ya que su aglomeración se calcula fácilmente como $CF_i+CF_j=(N_i+N_j,LS_i+LS_j,SS_i+SS_j)$.\n",
        "* La idea es construir el árbol de manera iterativa con base en un factor de ramificación B y un umbral T. Se parte de un nodo raíz al cuál se van agregando hijos ($child$'s) con base en una heurística.\n",
        "* Cada nodo no-hoja contiene cuando mucho B entradas, y un nodo hoja representa un cluster hecho de todos los subclusters representados por sus entradas de forma $[CF_i]$, donde $i=1,2,\\ldots,L$. Pero todas las entradas de un nodo hoja deben respetar el valor de umbral T, que representa la cota superior del diámetro (o radio) del cluster. El tamaño del árbol es función de T. Cuanto más grande, más pequeño el árbol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onKzt5_aUC-s"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import Birch\n",
        "\n",
        "modelo = Birch(threshold=0.01, n_clusters=3)\n",
        "\n",
        "modelo.fit(X)\n",
        "yhat = modelo.predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(yhat == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [OPTICS](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html)\n",
        "\n",
        "OPTICS (Ordering Points To Identify the Clustering Structure) está muy relacionado con DBSCAN, también encuentra puntos *nucleo* y expande a partir de ellos. A diferencia de DBSCAN, mantiene una jerarquía de clusters para un intervalo de radios pequeños. Es una mejor alternativa a DBSCAN en datasets grandes."
      ],
      "metadata": {
        "id": "Yk-Fo_ZkRGLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "\n",
        "modelo = OPTICS(max_eps=200, min_samples=9)\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "for cluster in clusters:\n",
        "    filas = np.where(yhat == cluster)\n",
        "    plt.scatter(X[filas, 0], X[filas, 1])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bDeYrKNhRFbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = []\n",
        "valores_eps = np.linspace(0.0001,200,100)\n",
        "\n",
        "for eps in valores_eps:\n",
        "    modelo = OPTICS(eps=eps, min_samples=5)\n",
        "    yhat = modelo.fit_predict(X)\n",
        "    clusters = np.unique(yhat)\n",
        "    num_clusters.append(len(clusters))\n",
        "\n",
        "plt.plot(valores_eps,num_clusters)\n",
        "plt.suptitle(\"Número de clusters en función del parámetro eps\")\n",
        "plt.xlabel(\"eps\",fontsize=15)\n",
        "plt.ylabel(\"Número de clusters\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e0BzdpuQSVXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY5OUxZbUC-u"
      },
      "source": [
        "### [Spectral Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)\n",
        "\n",
        "* La agrupación espectral es una clase general de métodos de agrupación, extraída del álgebra lineal.\n",
        "\n",
        "* \"_Una alternativa prometedora que ha surgido recientemente en varios campos es utilizar métodos espectrales para la agrupación. Aquí, uno usa los vectores propios más altos de una matriz derivada de la distancia entre puntos._\" [Andrew Y. Ng, Michael I. Jordan and Yair Weiss (2002). _On Spectral Clustering: Analysis and an algorithm_. In ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS.](https://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf)\n",
        "\n",
        "* Un review del método: https://link.springer.com/article/10.1007/s11222-007-9033-z\n",
        "\n",
        "* El hiperparámetro \"n_clusters\" es utilizado para especificar el número estimado de clústeres en los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knNzMY8vUC-u"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "modelo = SpectralClustering(n_clusters=2)\n",
        "\n",
        "yhat = modelo.fit_predict(X)\n",
        "\n",
        "clusters = np.unique(yhat)\n",
        "\n",
        "for cluster in clusters:\n",
        "    fila = np.where(yhat == cluster)\n",
        "    plt.scatter(X[fila, 0], X[fila, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Más métricas"
      ],
      "metadata": {
        "id": "woMLwJCXiqFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay más métricas que pueden usarse para el clustering. Las hay de dos tipos:\n",
        "\n",
        "1. Cuando no se conoce un clustering *ground truth*, en este caso se evalua el módelo unicamente con la información de él mismo. Por ejemplo:\n",
        "\n",
        "    * [Silhoutte score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)\n",
        "    * [Calinski-Harabasz Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score)\n",
        "    * [Davies-Bouldin Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score)\n",
        "    * ...\n",
        "\n",
        "2. Cuando se conoce un clustering *ground truth* o se quieren comparar dos clusterings. Por ejemplo:\n",
        "    * [AMI](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score)\n",
        "    * [V-measure](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score)\n",
        "    * [Rand index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html#sklearn.metrics.rand_score)\n",
        "    * ...\n"
      ],
      "metadata": {
        "id": "TwZ6xR8Ditvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Práctica"
      ],
      "metadata": {
        "id": "xd9VdumvETie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Puedes encontrar buenos clusterings para los siguientes datasets?\n",
        "\n",
        "Considera dos datasets DS1 y DS2/DS3. En cada uno de ellos, prueba los siguientes métodos:\n",
        "\n",
        "1. K-Means\n",
        "2. AgglomerativeClustering\n",
        "3. DBSCAN\n",
        "\n",
        "Tareas a realizar:\n",
        "\n",
        "* Realiza una busqueda de hiperparámetros basándote en alguna métrica como AMI, Silhoutte, Elbow Value (para el caso de K-Means) o alguna otra.\n",
        "* Con estos hiperparámetros, escoge el mejor clustering de cada uno de los tres métodos.\n",
        "* Compara visualmente los 3 clusterings obtenidos.\n",
        "* Usando el score de silueta y el índice Calinski-Harabasz, ¿cuál de los tres clusterings fue mejor?\n",
        "* En el caso del dataset DS1, tienes un *ground truth* clustering. Reporta el valor de la métrica [AMI](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score) y [ARI](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score)."
      ],
      "metadata": {
        "id": "U4pvJ24QjBeT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqCnMpbRUC-z"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons, make_blobs\n",
        "import numpy as np\n",
        "\n",
        "n_samples = 500\n",
        "\n",
        "DS1 = make_moons(n_samples=n_samples, noise=.05)\n",
        "DS2 = np.random.rand(n_samples, 2)\n",
        "DS3 = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=170)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# X, y = DS1\n",
        "X, y = DS3\n",
        "# X = DS2\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0],X[:,1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FrbW98Gz940R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GK6IZ8wOntck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficar los clusters:"
      ],
      "metadata": {
        "id": "Su_f19cQnuMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zepYvT6PUC-z"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1,2,figsize=(9,5),sharey=True)\n",
        "axs[0].scatter(X[:,0],X[:,1], c=y)\n",
        "axs[0].set_title(\"Original dataset\")\n",
        "axs[1].scatter(X[:,0],X[:,1], c=y_clusters)\n",
        "axs[1].set_title(\"Clustering\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 1: Documentos de Wikipedia\n",
        "\n",
        "Usaremos otra vez el dataset de documentos de Wikipedia, tomaremos la versión parcial y preprocesada de la sesión pasada.\n",
        "\n",
        "El objetivo de la práctica es segmentar los documentos en grupos con temáticas similares. Para esto, usaremos algoritmos de clustering. Si las representaciones vectoriales de los documentos son *buenas*, lograremos este objetivo."
      ],
      "metadata": {
        "id": "baK_yZkgTibB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud -qq"
      ],
      "metadata": {
        "id": "vy_MugDLg2Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/03%20Machine%20Learning/data/spanish-wikipedia-dataframe.csv\"\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "df"
      ],
      "metadata": {
        "id": "ON51TIO0Tj6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos quedamos con documentos que tengan entre 50 y 200 palabras"
      ],
      "metadata": {
        "id": "97lA4vxehL0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Palabras'] = df['Texto'].apply(lambda x: x.split())\n",
        "df['Total'] = df['Palabras'].apply(lambda x: len(x))\n",
        "df.drop(columns=['Palabras'],inplace=True)\n",
        "\n",
        "df = df[(df['Total'] < 200) & (df['Total'] > 50)]\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "Mk9O0MoXV5PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construimos la matriz BOW de los documentos y hacemos PCA para obtener una matriz de caracteristicas (numéricas continuas)."
      ],
      "metadata": {
        "id": "I6zM-rCnhQ3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "docs_list = df['Texto'].values\n",
        "\n",
        "cv = CountVectorizer(max_features=None)\n",
        "X_bow = cv.fit_transform(docs_list)\n",
        "print(X_bow.shape)\n",
        "\n",
        "pca = PCA(svd_solver='auto')\n",
        "X_pca = pca.fit_transform(np.asarray(X_bow.todense()))\n",
        "print(X_pca.shape)"
      ],
      "metadata": {
        "id": "g1r0VLlGUjN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tomamos las primeras `n_dim` componentes principales como representación vectorial de cada documento."
      ],
      "metadata": {
        "id": "65nedGJjhcnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_dim = 20\n",
        "\n",
        "X_pca_dim = X_pca[:,:n_dim]\n",
        "print(X_pca_dim.shape)"
      ],
      "metadata": {
        "id": "v1EwVtj_VY1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos clustering a las representaciones vectoriales."
      ],
      "metadata": {
        "id": "3Mtik-oKirjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "clustering = KMeans(n_clusters=3)\n",
        "clustering.fit(X_pca_dim)\n",
        "clusters = clustering.labels_"
      ],
      "metadata": {
        "id": "Pr19w-riUksN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idxs_per_cluster = {j:np.where(clusters==j)[0] for j in np.unique(clusters)}\n",
        "documents_per_cluster = {j:df.loc[idxs_per_cluster[j],'Texto'].values for j in np.unique(clusters)}\n",
        "\n",
        "documents_per_cluster"
      ],
      "metadata": {
        "id": "clyEjbRtVprv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la finalidad de explorar el contenido de los textos de cada cluster, hacemos una nube de palabras de los documentos de cada cluster. Para esto, usamos el módulo [wordcloud](https://pypi.org/project/wordcloud/).\n",
        "\n",
        "Aquí puedes ver [ejemplos de su uso](https://github.com/amueller/word_cloud/tree/main)"
      ],
      "metadata": {
        "id": "_RQzX5gShqcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=3,figsize=(15,5),dpi=100)\n",
        "for k,ax in enumerate(axs):\n",
        "    wordcloud = WordCloud().generate(\" \".join(documents_per_cluster[k]))\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(f\"Cluster {k}\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "WCmQvHOdd2WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print(f\"Score de silueta: {silhouette_score(X_pca_dim,clusters)}\")"
      ],
      "metadata": {
        "id": "bHXfJgDrjXg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓\n",
        "\n",
        "* ¿Puedes ver de qué tratan los documentos de cada cluster?\n",
        "* Prueba a cambiar el número de clusters.\n",
        "* Prueba a cambiar el número de dimensiones de las representaciones vectoriales.\n",
        "* Prueba a cambiar el método de clustering\n",
        "* En cada tarea de clustering, mide el coeficiente de silueta."
      ],
      "metadata": {
        "id": "HtwoFKxDiL15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 1: Canciones de Spotify"
      ],
      "metadata": {
        "id": "NrEN69yjS9Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este conjunto de datos contiene estadísticas de audio de las 2.000 canciones top de Spotify. Los datos contienen alrededor de 15 columnas que describen la canción y algunas de sus cualidades. Se incluyen canciones publicadas desde 1956 hasta 2019 de algunos artistas notables y famosos. Estos datos contienen características de audio como Danceability, BPM, Liveness, Valence(Positivity) y algunas más:\n",
        "\n",
        "* Índice: ID\n",
        "* Título: Nombre de la pista\n",
        "* Artista: Nombre del artista\n",
        "* Género superior: Género de la pista\n",
        "* Año: Año de lanzamiento de la pista\n",
        "* Pulsaciones por minuto (BPM): El tempo de la canción\n",
        "* Energy: La energía de una canción: cuanto más alto sea el valor, más energética será la canción.\n",
        "* Danceability: Cuanto más alto sea el valor, más fácil será bailar esta canción.\n",
        "* Loudness: Cuanto más alto sea el valor, más fuerte será la canción.\n",
        "* Liveness: ...\n",
        "* Valence: Cuanto más alto sea el valor, más positivo será el estado de ánimo de la canción.\n",
        "* Duración: La duración de la canción.\n",
        "* Acousticness: Cuanto más alto sea el valor, más acústica será la canción.\n",
        "* Speechiness: Cuanto más alto sea el valor, más palabras habladas contiene la canción.\n",
        "* Popularity: Cuanto más alto sea el valor, más popular es la canción.\n",
        "\n",
        "Este dataset se encuentra en [Kaggle](https://www.kaggle.com/datasets/iamsumat/spotify-top-2000s-mega-dataset)"
      ],
      "metadata": {
        "id": "YUSnLgDc6yxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'https://github.com/DCDPUAEM/DCDP/raw/main/03%20Machine%20Learning/data/spotify-2000.csv'\n",
        "df = pd.read_csv(url,index_col=0,thousands=',')\n",
        "df"
      ],
      "metadata": {
        "id": "2p5rJTnaEKAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df['Length (Duration)'].values)\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "AuqCCslwj11Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generos = df['Top Genre'].unique()\n",
        "print(f\"Hay {len(generos)} géneros únicos:\")\n",
        "print(generos)"
      ],
      "metadata": {
        "id": "FUbx_iOg_c7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "oRqQgbox70gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A manera de análisis exploratorio, veamos las correlaciones entre variables, ¿qué observamos?"
      ],
      "metadata": {
        "id": "U4-A0tlA9kRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from seaborn import heatmap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "correlaciones = df.iloc[:,3:].corr()\n",
        "heatmap(correlaciones)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5of0zRUs9I24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que algunos métodos de clustering son susceptibles a la escala de valores, hacemos un escalamiento."
      ],
      "metadata": {
        "id": "_hud5RYP7zhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "df2 = df[[\"Beats Per Minute (BPM)\", \"Loudness (dB)\",\n",
        "              \"Liveness\", \"Valence\", \"Acousticness\",\n",
        "              \"Speechiness\"]].copy()\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df2[df2.columns] = scaler.fit_transform(df2[df2.columns])\n",
        "X = df2.values\n",
        "\n",
        "df2.head(3)"
      ],
      "metadata": {
        "id": "N_Tb3EgWTljS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.describe()"
      ],
      "metadata": {
        "id": "Cdq-yECD-rBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos K-means para segmentar en 10 grupos"
      ],
      "metadata": {
        "id": "cK5a1QqrB03t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "modelo = KMeans(n_clusters=10, n_init='auto')\n",
        "\n",
        "modelo.fit(X)\n",
        "clusters = modelo.labels_\n",
        "\n",
        "print(f\"Las primeras 10 etiquetas: {clusters[:10]}\")"
      ],
      "metadata": {
        "id": "BC2tD2N28yVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integramos la información de los clusters al dataframe original."
      ],
      "metadata": {
        "id": "QGUA-MzoCCDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Music Segments\"] = clusters\n",
        "df[\"Music Segments\"] = df[\"Music Segments\"].map({0: \"Cluster 1\", 1:\n",
        "    \"Cluster 2\", 2: \"Cluster 3\", 3: \"Cluster 4\", 4: \"Cluster 5\",\n",
        "    5: \"Cluster 6\", 6: \"Cluster 7\", 7: \"Cluster 8\",\n",
        "    8: \"Cluster 9\", 9: \"Cluster 10\"})\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "rhHEN2zCUo0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observemos un cluster"
      ],
      "metadata": {
        "id": "dn_xNOGSl4vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster = 'Cluster 3'\n",
        "\n",
        "df[df['Music Segments']==cluster][['Artist','Title','Top Genre','Year']]"
      ],
      "metadata": {
        "id": "MCCBqzzVl6-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver los artistas en este cluster"
      ],
      "metadata": {
        "id": "YueDpOXR59u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Music Segments']==cluster]['Artist'].unique()"
      ],
      "metadata": {
        "id": "zEI8jGX86AUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficamos usando solamente 3 features. Usamos el módulo [plotly](https://plotly.com/python/) para gráficas interactivas.\n",
        "\n",
        "Otra alternativa es [Bokeh](https://bokeh.org/)."
      ],
      "metadata": {
        "id": "wcoMmKJDCHfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "PLOT = go.Figure()\n",
        "\n",
        "for i in list(df[\"Music Segments\"].unique()):\n",
        "    PLOT.add_trace(go.Scatter3d(x = df[df[\"Music Segments\"]==i]['Beats Per Minute (BPM)'],\n",
        "                                    y = df[df[\"Music Segments\"] ==i]['Energy'],\n",
        "                                    z = df[df[\"Music Segments\"] ==i]['Danceability'],\n",
        "                                    mode = 'markers',marker_size = 6, marker_line_width = 1,\n",
        "                                    name = str(i)))\n",
        "PLOT.update_traces(hovertemplate='Beats Per Minute (BPM): %{x} <br>Energy: %{y} <br>Danceability: %{z}')\n",
        "\n",
        "PLOT.update_layout(width = 800, height = 800, autosize = True, showlegend = True,\n",
        "                   scene = dict(xaxis=dict(title = 'Beats Per Minute (BPM)', titlefont_color = 'black'),\n",
        "                                yaxis=dict(title = 'Energy', titlefont_color = 'black'),\n",
        "                                zaxis=dict(title = 'Danceability', titlefont_color = 'black')),\n",
        "                   font = dict(family = \"Arial\", color  = 'black', size = 12))\n",
        "\n",
        "PLOT.show()"
      ],
      "metadata": {
        "id": "m1dXHIGFUG04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando sólo dos dimensiones:"
      ],
      "metadata": {
        "id": "7okOAKI4GRMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(dpi=120)\n",
        "for segment in df[\"Music Segments\"].unique():\n",
        "    plt.scatter(x = df[df[\"Music Segments\"]==segment]['Beats Per Minute (BPM)'],\n",
        "                y = df[df[\"Music Segments\"] ==segment]['Energy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2VhzrHq0FYFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(dpi=120)\n",
        "for segment in df[\"Music Segments\"].unique():\n",
        "    plt.scatter(x = df[df[\"Music Segments\"]==segment]['Danceability'],\n",
        "                y = df[df[\"Music Segments\"] ==segment]['Energy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B5nypRH6GVDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print(f\"Score de silueta: {silhouette_score(X,clusters)}\")"
      ],
      "metadata": {
        "id": "EKMSEGpXgP5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭕ Preguntas:\n",
        "* Siendo K-Means, ¿por qué se no se ve la separación perfecta?\n",
        "\n",
        "⭕ Ejercicio 1. Continuando con este método de K-Means:\n",
        "* ¿Qué valor de K es mejor? Puedes usar cualquiera de los 3 criteros de arriba, empezando por el *elbow value*.\n",
        "* Una vez que hayas escogido un valor para $K$, reportar los valores de las métricas de clustering: score de Silueta, [Calinski-Harabasz Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score) y [Davies-Bouldin Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
        "\n",
        "⭕ Ejercicio 2:\n",
        "\n",
        "* Repetir el experimento, ahora usando Agglomerative Clustering y DBSCAN.\n",
        "* ¿Puedes elevar las métricas de clustering? Considera las métricas score de Silueta y [Davies-Bouldin Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
        "* Explora visualmente algunas canciones de los clusters, ¿tiene sentido el agrupamiento?"
      ],
      "metadata": {
        "id": "ACYNyhKPefTU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2gwkihWG6Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 2: `20newsgroups`"
      ],
      "metadata": {
        "id": "0saXLbUX3uh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retomamos el corpus de documentos `20newsgroups`. Realizaremos un clustering en las representaciones de los documentos para ver qué documentos se agrupan juntos."
      ],
      "metadata": {
        "id": "TvuZKGxMXqlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Descargamos el dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "\n",
        "data_full = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),\n",
        "                               categories=['soc.religion.christian','sci.space','rec.autos'])"
      ],
      "metadata": {
        "id": "-1ujSDe-4xPx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = data_full.data\n",
        "print(f\"Número de documentos: {len(docs)}\")\n",
        "topics = data_full.target\n",
        "print(f\"Número de tópicos: {np.unique(topics).shape[0]}\")"
      ],
      "metadata": {
        "id": "Y5Z3lysw4-D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[:2]"
      ],
      "metadata": {
        "id": "TfektfBKjNd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Representaciones *term-document* y *tf-idf*\n",
        "\n",
        "Hemos visto que los documentos se pueden representar por medio de vectores *sparse* cuyas componentes son conteos de las apariciones de ciertas palabras.\n",
        "\n",
        "Otra forma parecida de representar documentos es por medio de vectores cuyas componentes ahora representan índices *tf-idf* (term frequency - inverse document frequency). Estas tienen la ventaja de restar importancia a palabras que aparecen en muchos documentos.\n",
        "\n",
        "Para un corpus $D$ de documentos, el índice $tfidf$ de un término $t$ en un documento $d$ se calcula como\n",
        "\n",
        "$$tfidf(t,d,D)=tf(t,d)idf(t,D)$$\n",
        "\n",
        "donde\n",
        "\n",
        "$$tf(t,d)=\\frac{f_{t,d}}{\\sum_{s\\in d}f_{s,d}}$$\n",
        "\n",
        "$$idf(t,D)=\\log\\frac{N}{|\\{d\\in D : t\\in d\\}|}$$\n",
        "\n",
        "$f_{t,d}$ es el número de ocurrencias de un término $t$ en un documento $d$, $N$ es el número de documentos en $D$."
      ],
      "metadata": {
        "id": "pJ2riMEvdhLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "corpus = [\n",
        "        'This is the first document.',\n",
        "        'This document is the second document.',\n",
        "        'And this is the third one.',\n",
        "        'Is this the first document?',\n",
        "        ]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "print(X_tfidf)\n",
        "print(X_tfidf.todense()[:,:3])"
      ],
      "metadata": {
        "id": "s1X2lUBMfxFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_counts = count_vectorizer.fit_transform(corpus)\n",
        "print(count_vectorizer.get_feature_names_out())\n",
        "print(X_counts)\n",
        "print(X_counts.todense()[:,:3])"
      ],
      "metadata": {
        "id": "9eph-7ZsglIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ],
      "metadata": {
        "id": "qe3PsrBrdqKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traemos la función para limpiar texto de hace algunas sesiones"
      ],
      "metadata": {
        "id": "nelHZO35lj9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/02-Machine-Learning/data/limpiador_texto.py\"\n",
        "!wget --no-cache --backups=1 {url}"
      ],
      "metadata": {
        "id": "O8hfDrERlecE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import download\n",
        "\n",
        "download('stopwords')\n",
        "download('punkt')"
      ],
      "metadata": {
        "id": "sm99y0I5lqlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from limpiador_texto import preprocesar_textos\n",
        "\n",
        "clean_docs = preprocesar_textos(docs)"
      ],
      "metadata": {
        "id": "MO7c9yE4l0iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos el clustering con las dos representaciones y calculamos las métricas de rendimiento del clustering."
      ],
      "metadata": {
        "id": "WwvyWWKmlw3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                             max_features=200)\n",
        "X_tfidf = vectorizer.fit_transform(docs)\n",
        "# X_tfidf = vectorizer.fit_transform(clean_docs)\n",
        "print(X_tfidf.shape)\n",
        "\n",
        "clustering = KMeans(n_clusters=3, n_init='auto')\n",
        "clustering.fit(X_tfidf)\n",
        "clusters = clustering.labels_\n",
        "\n",
        "print(f\"AMI: {adjusted_mutual_info_score(topics,clusters)}\")\n",
        "print(f\"AR: {adjusted_rand_score(topics,clusters)}\")\n",
        "print(f\"Silhoutte del clustering obtenido: {silhouette_score(X_tfidf,clusters)}\")\n",
        "print(f\"Silhoutte de los tópicos: {silhouette_score(X_tfidf,topics)}\")"
      ],
      "metadata": {
        "id": "0CTGrbYo6BwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english',\n",
        "                             max_features=200)\n",
        "# X_counts = vectorizer.fit_transform(docs)\n",
        "X_counts = vectorizer.fit_transform(clean_docs)\n",
        "print(X_counts.shape)\n",
        "\n",
        "clustering = KMeans(n_clusters=3, n_init='auto',random_state=57)\n",
        "clustering.fit(np.asarray(X_counts.todense()))\n",
        "clusters_counts = clustering.labels_\n",
        "\n",
        "print(f\"AMI: {adjusted_mutual_info_score(topics,clusters_counts)}\")\n",
        "print(f\"AR: {adjusted_rand_score(topics,clusters_counts)}\")\n",
        "print(f\"Silhoutte del clustering obtenido: {silhouette_score(np.asarray(X_counts.todense()),clusters_counts)}\")\n",
        "print(f\"Silhoutte de los tópicos: {silhouette_score(np.asarray(X_counts.todense()),topics)}\")"
      ],
      "metadata": {
        "id": "nKV0jP58FYYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podríamos explorar el comportamiento del parámetro `n_clusters` usando los criterios de intertia y silhoutte."
      ],
      "metadata": {
        "id": "xBGsfhT5hXVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sils = []\n",
        "inertias = []\n",
        "\n",
        "for n in range(2,20):\n",
        "    clustering = KMeans(n_clusters=n, n_init='auto')\n",
        "    clustering.fit(np.asarray(X_counts.todense()))\n",
        "    # sil = silhouette_score(np.asarray(X_counts.todense()),clustering.labels_)\n",
        "    # sils.append(sil)\n",
        "    inertias.append(clustering.inertia_)\n",
        "\n",
        "\n",
        "plt.plot(list(range(2,20)),inertias)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bpecWbud1DOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploración de los clusters"
      ],
      "metadata": {
        "id": "ukpKNSfcdsad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos probar algunas técnicas adicionales para explorar los clusters"
      ],
      "metadata": {
        "id": "VQz3WBlK5nuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq wordcloud"
      ],
      "metadata": {
        "id": "XkLYugf8Vn1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1,ncols=3,figsize=(15,10),dpi=100)\n",
        "for k,ax in enumerate(axs):\n",
        "    cluster_text = \" \".join([clean_docs[j] for j,cluster in enumerate(clusters_counts) if cluster==k])\n",
        "    wc = WordCloud().generate(cluster_text)\n",
        "    ax.set_title(f\"Cluster {k}\")\n",
        "    ax.imshow(wc, interpolation='bilinear')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "whFCsDYgYwp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imprimimos algunos documentos en cada cluster"
      ],
      "metadata": {
        "id": "6E8NOEer7UEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(43)\n",
        "\n",
        "for k in range(3):\n",
        "    docs_in_cluster = [clean_docs[j] for j,cluster in enumerate(clusters_counts) if cluster==k]\n",
        "    print(f\"Cluster {k} {20*'-'}\")\n",
        "    print(f\"Número de documentos en el cluster: {clusters_counts[clusters_counts==k].shape[0]}\")\n",
        "    some_docs = np.random.choice(docs_in_cluster,size=3)\n",
        "    print(some_docs)"
      ],
      "metadata": {
        "id": "iYxzthhrgTyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭕ **Ejercicio**\n",
        "\n",
        "Repite el experimento variando el parámetro `max_features` de los vectorizadores para ver si puedes subir las métricas de rendimiento y obtener una mejor separación de tópicos, esto hazlo *visualmente* (usando las nubes de palabras y algunos documentos).\n",
        "\n",
        "Usa los dos vectorizadores\n"
      ],
      "metadata": {
        "id": "PfeHRFHsmB-e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bODgImjXkmfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCaOm_vrd4qr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9RH5O2sb2awy",
        "pJ2riMEvdhLS"
      ],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}